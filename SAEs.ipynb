{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SAE Intro Project\n",
    "\n",
    "The following project and notebook is a simple introduction to the Sparse Auto Encoder (SAE). I do apologise for any rushed or 'malpractice' code, I have finals rn haha.\n",
    "Anyway, the project is split into two parts:\n",
    "1. A simple SAE implementation from scratch using Pytorch.\n",
    "2. A new SAE variant.\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Simple SAE Implementation\n",
    "\n",
    "The following chunk of code is just pre-requisite code for setting up the SAE. This step 1."
   ],
   "id": "66c614223808e8e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# imports and globals setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The LLM from which we shall build this SAE from\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "# The dataset we shall use to prompt the SAE while training. Originallt I wanted to use \"monology/pile-uncopyrighted\"\n",
    "# But this is too huge and I don't have enough space for this, so I will use a smaller dataset, which makes everything faster,\n",
    "# plus this is a proof-of-concept and this can be easily scaled.\n",
    "# dataset_name = \"monology/pile-uncopyrighted\"\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "\n",
    "# Store activations here, from the layer which we are interested in\n",
    "# Might be a better way to do this but this is the simplest\n",
    "stored_activations = []\n",
    "\n",
    "# This is the layer we are interested in, in the model.\n",
    "# NB 0-based index, so 5 is the 6th layer\n",
    "# Choice of layer is kinda arbitrary, however we want to choose something that is not too deep\n",
    "# and not too shallow. This is a good trade-off in interpretability plus the Anthropic paper\n",
    "# suggests that these \"middle\" layers are a decent choice to start.\n",
    "chosen_layer = 5\n",
    "\n",
    "# The maximum number of tokens the input is truncated to.\n",
    "# This is important for VRAM safety, especially on smaller GPUs\n",
    "MAX_LEN_TRUNC = 128\n",
    "\n",
    "# This is the coefficient for the sparsity loss term. This is a pseudo-hyperparameter that we can tune.\n",
    "# It is used to encourage the SAE to learn a sparse representation of the data, as always, this is kinda\n",
    "# arbitrary to begin with, however we can change this later. This value seems to work well for now.\n",
    "ENCOURAGEMENT_COEFF = 1e-3\n",
    "\n",
    "# This is the number of epochs we shall train the SAE for.\n",
    "SAE_TRAIN_EPOCHS = 1000\n",
    "\n",
    "# History trackers\n",
    "loss_history = []\n",
    "recon_history = []\n",
    "sparsity_history = []\n",
    "\n",
    "# This is the hook function, we shall call this during the forward pass\n",
    "# So we can  \"inspect\" the activations of the layer\n",
    "# by simply storing them in the global activations array\n",
    "def hook_fn(module, input, output):\n",
    "    stored_activations.append(output.detach().cpu())"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset and training prompts\n",
    "print(\"Loading dataset:\", dataset_name)\n",
    "dataset = load_dataset(dataset_name, split=\"train\", keep_in_memory=True)\n",
    "print(\"Dataset loaded\")\n",
    "prompts = [example[\"text\"] for example in dataset]\n",
    "print(\"Loaded dataset with\", len(prompts), \"prompts.\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"Loaded model:\", model_name)\n",
    "\n",
    "# Move model to GPU if available, using MPS cause I have a Mac\n",
    "# Hopefully this should auto-detect which device to use\n",
    "device = torch.device(\"mps\"\n",
    "                        if torch.backends.mps.is_available() else \"cuda\"\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device) # print for sanity\n",
    "\n",
    "# Move model to device and set to eval mode\n",
    "model.to(device).eval()\n",
    "print(\"Set model to eval mode.\")\n",
    "\n",
    "# Choose a layer â€” here, MLP from block 5\n",
    "target_layer = model.gpt_neox.layers[chosen_layer].mlp\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "print(\"Registered forward hook on layer:\", chosen_layer)"
   ],
   "id": "731b8b921aefb815",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we want to tokenize the prompts and run them through the model. Specifically, the goal of the next step\n",
    "is to run the model in inference mode, and store the activations of the layer we are interested in. This is so that we can\n",
    "later use these activations to train the SAE. The activations are stored in the `all_activations` variable. We grab this data from the kinda arbitrary layer we chose earlier."
   ],
   "id": "18c343a4b5a3dde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through all the prompts and tokenize them, saving the required activations\n",
    "print(\"Tokenizing prompts and running through model...\")\n",
    "for prompt in tqdm(prompts, desc=\"Tokenizing and running prompts\"):\n",
    "    # IMPORTANT TRUNCATE FOR VRAM SAFETY\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN_TRUNC).to(device)\n",
    "\n",
    "    # Since this is the forward pass, we are interested in the activations of the layer are not doing backprop\n",
    "    # We can disable gradient calculation to save memory\n",
    "    with torch.no_grad():\n",
    "        # Obliterate the current cache so we can only keep the activations we are interested in currently.\n",
    "        stored_activations.clear()\n",
    "        # Forward pass through the model\n",
    "        model(**inputs)\n",
    "\n",
    "        # Check if we have activations stored, from the forward hook from the forward pass.\n",
    "        if stored_activations:\n",
    "            # Grab middle token from first sequence\n",
    "            activ = stored_activations[0][0]  # shape: (seq_len, hidden_dim)\n",
    "            mid_token_index = activ.shape[0] // 2\n",
    "            middle_activation = activ[mid_token_index]  # shape: (hidden_dim,)\n",
    "\n",
    "            # Simple activation collector for later, check if it exists already, if not create it\n",
    "            if 'all_activations' not in locals():\n",
    "                all_activations = []\n",
    "            all_activations.append(middle_activation)\n",
    "\n",
    "print(\"Finished running prompts through model. Removing hook\")\n",
    "hook.remove()"
   ],
   "id": "79715cf515f9eb86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the activations we are interested in, we can now create and start to train the SAE. The following code is a simple implementation of the SAE. We can use the following relatively simple class definition with Pytorch to create the SAE. The class is a simple autoencoder with a linear encoder and decoder. The encoder takes the activations as input and outputs a lower-dimensional representation of the activations. The decoder takes this lower-dimensional representation and outputs the original activations.",
   "id": "a556d625890b0489"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fairly simplistic SAE implementation by deriving from base nn.Module\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Not too sure if linear transform is the right one to use here, will investigate later\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ReLU is the basic/standard one I think, the variants use something different?\n",
    "        z = torch.relu(self.encoder(x))\n",
    "\n",
    "        # Math convention calls the decoded, x_hat\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z"
   ],
   "id": "e584ff1afa989b78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stack activations into a single tensor then shove it into the model\n",
    "data = torch.stack(all_activations).to(device)\n",
    "sae = SAE(input_dim=data.shape[1], hidden_dim=1024).to(device)\n",
    "optimizer = optim.Adam(sae.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ],
   "id": "1e20e7e4ba59b8c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have a configured SAE, we want to train it. The training process is relatively simple, we use the MSE loss function to calculate the reconstruction loss between the input and output of the SAE. We also add a sparsity loss term to encourage the SAE to learn a sparse representation of the data. The sparsity loss is simply the mean of the absolute values of the activations of the encoder. This encourages the SAE to learn a sparse representation of the data, which is what we want.",
   "id": "ea659e76416fa52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start training the SAE\n",
    "for epoch in tqdm(range(SAE_TRAIN_EPOCHS), desc=\"Training model\"):\n",
    "    sae.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    recon, codes = sae(data)\n",
    "    recon_loss = loss_fn(recon, data)\n",
    "    sparsity_loss = codes.abs().mean()\n",
    "    loss = recon_loss + ENCOURAGEMENT_COEFF * sparsity_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track histories for very nice graph at the end\n",
    "    loss_history.append(loss.item())\n",
    "    recon_history.append(recon_loss.item())\n",
    "    sparsity_history.append(sparsity_loss.item())\n",
    "\n",
    "    # This is a massive slowdown but lowkey useful for debugging\n",
    "    # print(f\"Epoch {epoch}: Total={loss.item():.4f}, Recon={recon_loss.item():.4f}, Sparsity={sparsity_loss.item():.4f}\")"
   ],
   "id": "97c9c2e766d85e7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 Results\n",
    "\n",
    "__NB. The following plots are fairly standard matplotlib boilerplate code so I won't explain the actual code too much, just the results.__"
   ],
   "id": "6183b8d44c852b29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Activations Histogram\n",
    "This is the code activations histogram after training.\n",
    "The goal for the SAE is to have a distribution that is fairly sparse, with a few activations as it should \"disentangle\" semantics and should be mono-semantic in nature. Each neuron should ideally represent a single concept. Hence, logically, the ideal result for neurons to mostly be off during interpretation. Hence, the result should massively skew to 0."
   ],
   "id": "eea1ad12d2656552"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sae.eval()\n",
    "_, final_codes = sae(data)\n",
    "final_codes = final_codes.detach().cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(final_codes.flatten(), bins=100, range=(0, 1), log=True)\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Histogram of SAE Code Activations\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "800679aa8a9c6971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The sparsity distribution per sample\n",
    "This is the number of active units per sample and should be fairly \"unique\" in a sense that, there should be one massive spike and then significantly lower values surrounding it. There is should only be a small number of features that are active at any given time, indicating sparsity and that the SAE is working and learning a sparse representation of the data."
   ],
   "id": "42e13ae9b252abd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "binary_codes = (final_codes > ENCOURAGEMENT_COEFF).astype(int)\n",
    "sparsity_per_sample = binary_codes.sum(axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sparsity_per_sample, bins=30)\n",
    "plt.xlabel(\"Number of Active Units\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Sparsity Distribution Per Sample\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "23cd938bd7b78576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss History\n",
    "This is the loss history for the training process. The total loss should be decreasing over time, and the reconstruction loss should be significantly lower than the sparsity loss. This indicates that the SAE is learning a good representation of the data and that the sparsity loss is not dominating the training process. This is also just standard matplotlib boilerplate code so I won't explain the actual code too much."
   ],
   "id": "f2df105785093c51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history, label=\"Total Loss\")\n",
    "plt.plot(recon_history, label=\"Reconstruction Loss\")\n",
    "plt.plot(sparsity_history, label=\"Sparsity Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SAE Training Losses\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "28e1a662bbc856a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It took me probably like 10-11 hours to get here, 80% of which was reading research and the anthropic articles. Next, I will create an SAE variant upon the previous logic/implementation.",
   "id": "7fae02969fdfca50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2: SAE Variant\n",
    "\n",
    "This is my SAE variant, I will be basing it on a XXXXXXXXXXX variant and this is the following code. Very little can be changed from the above globals and data, only a new class has to be created and the training loop has to be changed. The rest is the same."
   ],
   "id": "64a61f0011bc8b1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8414385aee64f30c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
