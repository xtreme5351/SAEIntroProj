{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SAE Intro Project\n",
    "\n",
    "The following project and notebook is a simple introduction to the Sparse Auto Encoder (SAE). I do apologise for any rushed or 'malpractice' code, I have finals rn haha.\n",
    "Anyway, the project is split into two parts:\n",
    "1. A simple SAE implementation from scratch using Pytorch.\n",
    "2. A new SAE variant.\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# imports and globals setup\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The LLM from which we shall build this SAE from\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "# The dataset we shall use to prompt the SAE while training. Originallt I wanted to use \"monology/pile-uncopyrighted\"\n",
    "# But this is too huge and I don't have enough space for this, so I will use a smaller dataset, which makes everything faster,\n",
    "# plus this is a proof-of-concept and this can be easily scaled.\n",
    "# dataset_name = \"monology/pile-uncopyrighted\"\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "\n",
    "# Store activations here, from the layer which we are interested in\n",
    "# Might be a better way to do this but this is the simplest\n",
    "stored_activations = []\n",
    "\n",
    "# This is the layer we are interested in, in the model.\n",
    "# NB 0-based index, so 5 is the 6th layer\n",
    "# Choice of layer is kinda arbitrary, however we want to choose something that is not too deep\n",
    "# and not too shallow. This is a good trade-off in interpretability plus the Anthropic paper\n",
    "# suggests that these \"middle\" layers are a decent choice to start.\n",
    "chosen_layer = 5\n",
    "\n",
    "# The maximum number of tokens the input is truncated to.\n",
    "# This is important for VRAM safety, especially on smaller GPUs\n",
    "MAX_LEN_TRUNC = 128\n",
    "\n",
    "# This is the hook function, we shall call this during the forward pass\n",
    "# So we can  \"inspect\" the activations of the layer\n",
    "# by simply storing them in the global activations array\n",
    "def hook_fn(module, input, output):\n",
    "    stored_activations.append(output.detach().cpu())"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Simple SAE Implementation\n",
    "\n",
    "The following chunk of code is just pre-requisite code for setting up the SAE. This step 1."
   ],
   "id": "66c614223808e8e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset and training prompts\n",
    "print(\"Loading dataset:\", dataset_name)\n",
    "dataset = load_dataset(dataset_name, split=\"train\", keep_in_memory=True)\n",
    "print(\"Dataset loaded\")\n",
    "prompts = [example[\"text\"] for example in dataset]\n",
    "print(\"Loaded dataset with\", len(prompts), \"prompts.\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"Loaded model:\", model_name)\n",
    "\n",
    "# Move model to GPU if available, using MPS cause I have a Mac\n",
    "# Hopefully this should auto-detect which device to use\n",
    "device = torch.device(\"mps\"\n",
    "                        if torch.backends.mps.is_available() else \"cuda\"\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device) # print for sanity\n",
    "\n",
    "# Move model to device and set to eval mode\n",
    "model.to(device).eval()\n",
    "print(\"Set model to eval mode.\")\n",
    "\n",
    "# Choose a layer â€” here, MLP from block 5\n",
    "target_layer = model.gpt_neox.layers[chosen_layer].mlp\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "print(\"Registered forward hook on layer:\", chosen_layer)"
   ],
   "id": "731b8b921aefb815",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we want to tokenize the prompts and run them through the model. Specifically, the goal of the next step\n",
    "is to run the model in inference mode, and store the activations of the layer we are interested in. This is so that we can\n",
    "later use these activations to train the SAE. The activations are stored in the `all_activations` variable. We grab this data from the kinda arbitrary layer we chose earlier."
   ],
   "id": "18c343a4b5a3dde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through all the prompts and tokenize them, saving the required activations\n",
    "for prompt in prompts:\n",
    "    # IMPORTANT TRUNCATE FOR VRAM SAFETY\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN_TRUNC).to(device)\n",
    "\n",
    "    # Since this is the forward pass, we are interested in the activations of the layer are not doing backprop\n",
    "    # We can disable gradient calculation to save memory\n",
    "    with torch.no_grad():\n",
    "        # Obliterate the current cache so we can only keep the activations we are interested in currently.\n",
    "        stored_activations.clear()\n",
    "        # Forward pass through the model\n",
    "        model(**inputs)\n",
    "\n",
    "        # Check if we have activations stored, from the forward hook from the forward pass.\n",
    "        if stored_activations:\n",
    "            # Grab middle token from first sequence\n",
    "            activ = stored_activations[0][0]  # shape: (seq_len, hidden_dim)\n",
    "            mid_token_index = activ.shape[0] // 2\n",
    "            middle_activation = activ[mid_token_index]  # shape: (hidden_dim,)\n",
    "\n",
    "            # Simple activation collector for later, check if it exists already, if not create it\n",
    "            if 'all_activations' not in locals():\n",
    "                all_activations = []\n",
    "            all_activations.append(middle_activation)\n",
    "\n",
    "hook.remove()"
   ],
   "id": "79715cf515f9eb86",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
